{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9aaf9ce-43b6-46a5-a9a9-12537246b8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders in the BAM directory: 1490\n",
      "Number of IDs in the input file: 1490\n",
      "Number of folders not in the input file: 0\n",
      "\n",
      "Folders available in the BAM directory but not in the input file:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Paths to the directory and the input file\n",
    "bam_dir = \"/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/bams/\"\n",
    "input_file = \"./steps_inputs/step1/ENCODE_snatac_pseudobulk_replacements_v2_experiemnt_id_bam_url.txt\"\n",
    "\n",
    "# Get folder names in the BAM directory\n",
    "bam_folders = set(os.listdir(bam_dir))\n",
    "\n",
    "# Read the first column from the input file\n",
    "with open(input_file, \"r\") as file:\n",
    "    file_ids = set(line.split()[0] for line in file if line.strip())  # Extract the first column\n",
    "\n",
    "# Find BAM folders not present in the input file\n",
    "folders_not_in_file = bam_folders - file_ids\n",
    "\n",
    "# Print counts\n",
    "print(f\"Number of folders in the BAM directory: {len(bam_folders)}\")\n",
    "print(f\"Number of IDs in the input file: {len(file_ids)}\")\n",
    "print(f\"Number of folders not in the input file: {len(folders_not_in_file)}\")\n",
    "\n",
    "# Output the results\n",
    "print(\"\\nFolders available in the BAM directory but not in the input file:\")\n",
    "for folder in sorted(folders_not_in_file):\n",
    "    print(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a183cd21-0daa-4369-ace3-91c4d5575db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders in the BAM directory: 1490\n",
      "Number of IDs in the input file: 1490\n",
      "Number of BAM folders not in the input file: 0\n",
      "Number of ENCSR_IDs from the input file missing corresponding sorted BAM files: 0\n",
      "\n",
      "Folders available in the BAM directory but not in the input file:\n",
      "\n",
      "ENCSR_IDs and ENCFF_IDs from the input file that are missing corresponding sorted BAM files:\n",
      "\n",
      "Sub-file with missing unsorted BAM file paths created at: ./steps_inputs/step2/missing_bam_unsorted_files.txt\n",
      "Sub-file with missing BAM file rows created at: ./steps_inputs/step1/missing_bam_files.txt\n",
      "\n",
      "First two rows from ./steps_inputs/step1/missing_bam_files.txt:\n",
      "\n",
      "\n",
      "First two rows from ./steps_inputs/step2/missing_bam_unsorted_files.txt:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Paths to the directory and the input file\n",
    "bam_dir = \"/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/bams/\"\n",
    "input_file = \"./steps_inputs/step1/ENCODE_snatac_pseudobulk_replacements_v2_experiemnt_id_bam_url.txt\"\n",
    "output_bam_subfile = \"./steps_inputs/step1/missing_bam_files.txt\"  # Output file for missing unsorted BAM rows\n",
    "output_subfile = \"./steps_inputs/step2/missing_bam_unsorted_files.txt\"  # Output file for missing unsorted BAM rows\n",
    "\n",
    "# Get folder names in the BAM directory (subdirectories are ENCSR_IDs)\n",
    "bam_folders = {folder for folder in os.listdir(bam_dir) if os.path.isdir(os.path.join(bam_dir, folder))}\n",
    "\n",
    "# Read the input file and map the ENCSR_ID and ENCFF_ID (extracted from the URL) to their respective rows\n",
    "input_data = {}\n",
    "with open(input_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            columns = line.split()\n",
    "            encsr_id = columns[0]\n",
    "            encff_url = columns[1]  # Assuming the second column contains the URL\n",
    "\n",
    "            # Extract ENCFF_ID from the URL (after the last '/')\n",
    "            encff_id = encff_url.split('/')[-1].split('.')[0]  # This assumes ENCFF_ID is the part before '.bam'\n",
    "            \n",
    "            input_data[encsr_id] = {'encff_id': encff_id, 'line': line.strip()}\n",
    "\n",
    "# Find ENCSR_IDs from the input file that do not have a corresponding sorted BAM file in the BAM directory\n",
    "missing_sorted_bams = set()\n",
    "missing_bam_rows = []\n",
    "\n",
    "for encsr_id, data in input_data.items():\n",
    "    encff_id = data['encff_id']\n",
    "    # Use glob to find the *_sorted.bam file in the folder corresponding to ENCSR_ID\n",
    "    bam_file_pattern = os.path.join(bam_dir, encsr_id, f\"{encff_id}_sorted.bam\")\n",
    "    bam_files = glob.glob(bam_file_pattern)\n",
    "    \n",
    "    # If no matching sorted BAM file is found, add to missing_sorted_bams\n",
    "    if not bam_files:\n",
    "        missing_sorted_bams.add((encsr_id, encff_id))  # Store both ENCSR_ID and ENCFF_ID\n",
    "        missing_bam_rows.append(data['line'])  # Keep track of the missing BAM file row for output\n",
    "\n",
    "# Create the sub-file with rows for missing unsorted BAM files\n",
    "with open(output_subfile, \"w\") as output_file:\n",
    "    for encsr_id, encff_id in missing_sorted_bams:\n",
    "        if encsr_id in input_data:\n",
    "            # Construct the path for the missing unsorted BAM file\n",
    "            unsorted_bam_path = os.path.join(bam_dir, encsr_id, f\"{encff_id}_unsorted.bam\")\n",
    "            output_file.write(f\"{unsorted_bam_path}\\n\")  # Write the path to the unsorted BAM file\n",
    "\n",
    "# Create the sub-file for missing BAM rows from the input file\n",
    "with open(output_bam_subfile, \"w\") as output_file:\n",
    "    for row in missing_bam_rows:\n",
    "        output_file.write(f\"{row}\\n\")\n",
    "\n",
    "# Print counts\n",
    "print(f\"Number of folders in the BAM directory: {len(bam_folders)}\")\n",
    "print(f\"Number of IDs in the input file: {len(input_data)}\")\n",
    "print(f\"Number of BAM folders not in the input file: {len(bam_folders - set(input_data))}\")\n",
    "print(f\"Number of ENCSR_IDs from the input file missing corresponding sorted BAM files: {len(missing_sorted_bams)}\")\n",
    "\n",
    "# Output the results\n",
    "print(\"\\nFolders available in the BAM directory but not in the input file:\")\n",
    "for folder in sorted(bam_folders - set(input_data)):\n",
    "    print(folder)\n",
    "\n",
    "print(\"\\nENCSR_IDs and ENCFF_IDs from the input file that are missing corresponding sorted BAM files:\")\n",
    "for encsr_id, encff_id in sorted(missing_sorted_bams):\n",
    "    print(f\"{encsr_id} - {encff_id}\")\n",
    "\n",
    "print(f\"\\nSub-file with missing unsorted BAM file paths created at: {output_subfile}\")\n",
    "print(f\"Sub-file with missing BAM file rows created at: {output_bam_subfile}\")\n",
    "\n",
    "# Print the first 2 rows from each output file\n",
    "def print_first_two_rows(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            print(f\"\\nFirst two rows from {file_path}:\")\n",
    "            print(\"\".join(lines[:2]) if len(lines) >= 2 else \"\".join(lines))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{file_path} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# Print the first two rows of the output files\n",
    "print_first_two_rows(output_bam_subfile)\n",
    "print_first_two_rows(output_subfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd1fd21-229d-40d1-a861-21cba5713a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing ENCSR_ID rows have been saved to ./steps_inputs/step3/missing_atac_pseudobulk_new_peaks_files_mapping.txt.\n"
     ]
    }
   ],
   "source": [
    "#preperation for step 3:\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "# Define file paths\n",
    "input_file = \"./steps_inputs/step3/atac_pseudobulk_new_peaks_files_mapping.txt\"\n",
    "output_file = \"./steps_inputs/step3/missing_atac_pseudobulk_new_peaks_files_mapping.txt\"\n",
    "extra_ids_file = \"./steps_inputs/step3/extra_peaks_ids_not_in_input_file.txt\"\n",
    "peaks_dir = \"/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/peaks\"\n",
    "\n",
    "# Ensure the input file exists\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"Input file {input_file} does not exist. Exiting.\")\n",
    "    exit(1)\n",
    "\n",
    "# Collect ENCSR_IDs from the input file\n",
    "input_ids = set()\n",
    "with open(input_file, \"r\") as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        encsr_id = os.path.basename(row['ID'].strip('/'))\n",
    "        input_ids.add(encsr_id)\n",
    "\n",
    "# Collect ENCSR_IDs from the peaks_dir\n",
    "peaks_ids = set(next(os.walk(peaks_dir))[1])\n",
    "\n",
    "# Identify extra IDs in peaks_dir not present in input file\n",
    "extra_ids = peaks_ids - input_ids\n",
    "\n",
    "# Write extra IDs to a separate file\n",
    "with open(extra_ids_file, \"w\") as extra_file:\n",
    "    extra_file.write(\"ENCSR_ID\\n\")\n",
    "    for encsr_id in sorted(extra_ids):\n",
    "        extra_file.write(f\"{encsr_id}\\n\")\n",
    "\n",
    "# Open the input file and output file for missing rows\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\", newline=\"\") as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
    "\n",
    "    # Write the header to the output file\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Iterate through each row in the input file\n",
    "    for row in reader:\n",
    "        # Extract ENCSR_ID from the ID column\n",
    "        encsr_id = os.path.basename(row['ID'].strip('/'))\n",
    "\n",
    "        # Construct the search pattern for .bed.gz files\n",
    "        search_pattern = os.path.join(peaks_dir, encsr_id, \"*\", \"*.bed.gz\")\n",
    "\n",
    "        # Check if any matching files exist\n",
    "        if not glob.glob(search_pattern):\n",
    "            # Write the missing row to the output file\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(f\"Missing ENCSR_ID rows have been saved to {output_file}.\")\n",
    "print(f\"Extra ENCSR_IDs in {peaks_dir} not found in the input file have been saved to {extra_ids_file}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27686f62-45e4-4647-bee4-1be14b3b513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !squeue --user=$USER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b64450-1207-428a-af39-c3d63deaf0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "!squeue --user=$USER | wc -l\n",
    "# 483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ca5932c-d47b-422d-8395-f82f2bbcc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls -d $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_data/bams/*/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6db04da-325b-4075-9e80-9d22da222c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_data/bams/*/ | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6c29287-65e0-49f6-a7d4-0ded0f12abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls -d $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_data/peaks/*/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "48948a71-3cf9-44d6-b439-11659d87c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls -d $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_data/peaks/*/ | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99d37c98-61d7-42a1-964a-247b0d94362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lt $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_data/peaks/*/*/*_*.bed.gz | wc -l\n",
    "#1490\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "306d802e-7e41-4125-8297-a09095b23f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7350\n"
     ]
    }
   ],
   "source": [
    "# negative bed files\n",
    "!ls -lt $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_negative/*/*/*/fold_*/*nonpeaks_negatives.bed | wc -l\n",
    "# 7224\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9a9d6a17-de48-4482-a1a1-1285fe5265eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7450"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1490*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a534d6ed-3236-4923-87b5-05e871dac956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490\n"
     ]
    }
   ],
   "source": [
    "!ls -lt $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_negative/*/*/*/fold_0/*nonpeaks_negatives.bed | wc -l\n",
    "# 1490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a7043188-2e35-4fcc-b4e3-c76a25cc8ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490\n"
     ]
    }
   ],
   "source": [
    "!ls -lt $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_negative/*/*/*/fold_1/*nonpeaks_negatives.bed | wc -l\n",
    "# 1490\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ce206665-cbca-4cae-8a1e-b98514ffc177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490\n"
     ]
    }
   ],
   "source": [
    "!ls -lt $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_negative/*/*/*/fold_2/*nonpeaks_negatives.bed | wc -l\n",
    "# 1490\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3d067478-368f-4c45-ba89-9a9400a4cbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490\n"
     ]
    }
   ],
   "source": [
    "!ls -lt $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_negative/*/*/*/fold_3/*nonpeaks_negatives.bed | wc -l\n",
    "# 1432\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0587dc0d-9a4d-4a0f-9b43-3d72e7472bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490\n"
     ]
    }
   ],
   "source": [
    "!ls -lt $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_negative/*/*/*/fold_4/*nonpeaks_negatives.bed | wc -l\n",
    "# 1490\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a64fd11-8a25-45ee-ab2a-58b595ddf2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "!squeue --user=$USER | wc -l\n",
    "\n",
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a2746f-7ae2-40b7-8421-b86466c972f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "!squeue --user=$USER | sort -k1,1n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90ef09cc-caa1-49f9-b8b2-9c0234fd0005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/bams/ENCSR110XMU/ENCFF553COV.bed.gz_unsorted.bam: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/bams/ENCSR110XMU/ENCFF553COV.bed.gz_unsorted.bam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004b2b4e-6e10-4a11-bc07-3b286b2e60e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ./local_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99f423-663b-44d6-970b-48bb348581d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scancel 60411410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbfeb11-81b5-4c04-b3d8-0cb2d19eb101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ./steps_inputs/step5/peaks_filtered_by_blacklist_merged_with_organism_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1294d4af-3349-4589-a72b-920ab01e310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head -2 ./steps_inputs/step5/peaks_filtered_by_blacklist_merged_with_organism_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1bf5dc9-5642-4b56-8431-d18be3ac387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tail -2 ./local_logs/step5.NegativesNoPeaksBackground.combined.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0406934e-ee09-458a-8b78-b25d01fa18d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    " # execution\n",
    "!squeue --user=$USER | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbb43536-15f9-4aa8-b6ef-ffcabd1207eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# !squeue --user=$USER | grep step5Neg | wc -l\n",
    "!squeue --user=$USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83ce2421-30d4-43dd-8af6-5c7446e3b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scancel --user=$USER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8eda8f0-742d-48ad-8816-bef15dfa9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !squeue --user=$USER\n",
    "# !squeue --user=$USER | sort -k1,1n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d75b90-6d61-4ad9-bbe4-8ede68daabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm ./local_logs/step5.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43c180c-4e58-47f6-a035-f972d583b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scancel 63790365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec5f63a-e1a6-4c71-b312-59330f37c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scancel -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a55c113f-af27-4cec-8d80-736e648b556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# while true; do\n",
    "#     squeue --user=$USER\n",
    "#     sleep 120\n",
    "# done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b763055-e307-47a7-acee-054fbfbb3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scancel 57899498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43fa34e6-4929-475c-8efe-2b78c288ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm ./local_logs/633*\n",
    "# !./execute_sbatch_arrays_on_sherlock.sh ./steps_inputs/step6_3/debug_one_modisco_results_profile_scores_h5.txt step6-3-3-qc-bias-tn5.sh 1 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aab920bd-90e1-44e2-9643-9d1165d93003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !grep \"qval\" ./local_logs/slurm.step62.bpnetPipeline.combined.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0381a889-b737-4eb1-8cb4-9d62ac5d5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wc -l ./steps_inputs/step6/chrombpnet_pipeline_extracted_paths.txt\n",
    "# !head -5 ./local_logs/slurm.step62.bpnetPipeline.combined.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da74cf44-253c-4926-b1c8-b6197825d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tail -5 ./local_logs/slurm.step62.bpnetPipeline.combined.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f494c4fe-b853-44d5-9072-5f794d5e98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !grep modisco ./local_logs/slurm.step62.bpnetPipeline.combined.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ef5c379-a6aa-4f3a-a1de-cd6fa9745aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !grep \"qval\" ./local_logs/slurm.step62.bpnetPipeline.combined.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d8ea066-f383-43b4-83d3-cd417ec62ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head -5 ./local_logs/slurm.step62.bpnetPipeline.combined.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "965e34bb-8a80-4f99-a68e-daf750d8b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tail -5 ./local_logs/slurm.step62.bpnetPipeline.combined.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08016a6e-3f6b-44c0-ba36-faf5a13662f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scancel 58205153\n",
    "# !scancel -u $USER\n",
    "# !ls ./steps_inputs/step6/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4a80fc2f-91ed-4716-b31e-2ddca8cdd593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818\n"
     ]
    }
   ],
   "source": [
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_0/step62.chrombpnet/auxiliary/interpret_subsample/modisco_results_profile_scores.h5  | wc -l\n",
    "# 818\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e8560dc5-3a7e-46cb-a02f-3c0232be5603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fold 1:\n",
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_1/step62.chrombpnet/auxiliary/interpret_subsample/modisco_results_profile_scores.h5  | wc -l\n",
    "# 412\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f3dc682d-bdd0-4963-8e4b-730ee1dd335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fold 2:\n",
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_2/step62.chrombpnet/auxiliary/interpret_subsample/modisco_results_profile_scores.h5  | wc -l\n",
    "# 271\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "759c2450-8f54-4938-abd9-3236346c5155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fold 3:\n",
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_3/step62.chrombpnet/auxiliary/interpret_subsample/modisco_results_profile_scores.h5  | wc -l\n",
    "# 163\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "17ef58e3-feca-4141-81c9-8fd0464008b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fold 4:\n",
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_4/step62.chrombpnet/auxiliary/interpret_subsample/modisco_results_profile_scores.h5  | wc -l\n",
    "# 141\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "759bf51b-b5b9-4540-8bff-c372f36ed811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1807\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# all folds:\n",
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_*/step62.chrombpnet/auxiliary/interpret_subsample/modisco_results_profile_scores.h5  | wc -l\n",
    "# 1805\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52247bf1-aad2-4aa5-986e-41c9bba018ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5416/2134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "41914edf-9726-441c-a5ae-55a74191c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scancel -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66ce8732-61d7-458c-a6fa-de4c26c5b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !squeue -u $USER -o \"%i %j\" | grep step5Neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "224f0409-6634-4be7-bba2-1303834e7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scp -r eila@login.sherlock.stanford.edu:/scratch/users/eila/encode_pseudobulks_model_training/human/ENCSR037JDN/ENCFF933KCP/fold_1_30000_20240912_182023/evaluation . \n",
    "\n",
    "# !rm ./local_logs/slurm_samools_err.combined.err\n",
    "# !rm ./local_logs/slurm_samools_out.combined.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ed9ed22-1c14-4b08-9411-c37a4af0e3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execute_sbatch_arrays_on_sherlock-Copy1.sh\n",
      "execute_sbatch_arrays_on_sherlock.sh\n",
      "execute_sbatch_arrays_on_sherlock_try.sh\n"
     ]
    }
   ],
   "source": [
    "!ls execute*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "042dfbb6-7c20-486b-baff-fe35da1c318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "find /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/*/*/fold_0/step62.bpnetPipeline/evaluation/ -name \"overall_report.html\" \\\n",
    "    | while read dir; do\n",
    "        # Get the parent directory of \"evaluation\"\n",
    "        parent_dir=$(dirname \"$(dirname \"$dir\")\")\n",
    "        \n",
    "        # Check if modisco_results_profile_scores.h5 exists under auxiliary/interpret_subsample\n",
    "        if ! find \"$parent_dir/auxiliary/interpret_subsample\" -name \"modisco_results_profile_scores.h5\" > /dev/null; then\n",
    "            # Print the directory containing overall_report.html if modisco_results_profile_scores.h5 is not found\n",
    "            echo \"$parent_dir\"\n",
    "        fi\n",
    "    done\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06be2332-3a54-4e9a-baba-9e61ec1a065b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories missing at least one of the files ('overall_report.html' or 'modisco_results_profile_scores.h5'):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Set the base directories for the two files\n",
    "evaluation_dir = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/*/*/fold_0/step62.bpnetPipeline/evaluation'\n",
    "modisco_dir = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/*/*/fold_0/step62.bpnetPipeline/auxiliary/interpret_subsample'\n",
    "\n",
    "# Find all directories with the relevant files\n",
    "overall_report_dirs = set(os.path.dirname(path) for path in glob.glob(f\"{evaluation_dir}/overall_report.html\"))\n",
    "modisco_dirs = set(os.path.dirname(path) for path in glob.glob(f\"{modisco_dir}/modisco_results_profile_scores.h5\"))\n",
    "\n",
    "# Get the base path up to fold_0/step62.bpnetPipeline/\n",
    "base_path = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/*/*/fold_0/step62.bpnetPipeline'\n",
    "\n",
    "# Find directories missing at least one file\n",
    "missing_files_dirs = set()\n",
    "\n",
    "# Iterate over directories up to the base path level\n",
    "for dir in glob.glob(f\"{base_path}/*/*/\"):\n",
    "    # If the directory has overall_report.html but not modisco_results_profile_scores.h5\n",
    "    if dir in overall_report_dirs and dir not in modisco_dirs:\n",
    "        missing_files_dirs.add(dir)\n",
    "    # If the directory has modisco_results_profile_scores.h5 but not overall_report.html\n",
    "    elif dir in modisco_dirs and dir not in overall_report_dirs:\n",
    "        missing_files_dirs.add(dir)\n",
    "\n",
    "# Print out the result\n",
    "print(f\"Directories missing at least one of the files ('overall_report.html' or 'modisco_results_profile_scores.h5'):\")\n",
    "for dir in missing_files_dirs:\n",
    "    print(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ce1ea64-b4a7-4529-90cb-3503e3073f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_folders_without_file(base_path, target_file):\n",
    "    # Walk through the directory structure\n",
    "    folders_without_file = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        # Check if the current directory contains the target file\n",
    "        if target_file not in files:\n",
    "            folders_without_file.append(root)\n",
    "    \n",
    "    return folders_without_file\n",
    "\n",
    "# Define the base path and the target file\n",
    "base_path = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_0/'\n",
    "target_file = 'step62.bpnetPipeline/auxiliary/interpret_subsample/modisco_results_profile_scores.h5'\n",
    "\n",
    "# Call the function to find folders without the file\n",
    "folders = find_folders_without_file(base_path, target_file)\n",
    "\n",
    "# Print the results\n",
    "for folder in folders:\n",
    "    print(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5c0a1dc-6670-463d-a1bd-e9ed4b8c7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR000NVR/ENCFF585MYS/fold_0/step62.bpnetPipeline/auxiliary/interpret_subsample/modisco_results_profile_scores.h5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c96e71f-9437-4b85-a614-d16dc38df635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492\n"
     ]
    }
   ],
   "source": [
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/peaks | wc -l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41fd1199-70af-417a-b76a-88529f1ec3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR000NVR/ENCFF585MYS/fold_0/step62.bpnetPipeline/auxiliary/interpret_subsample/modisco_results_profile_scores.h5\n"
     ]
    }
   ],
   "source": [
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR000NVR/ENCFF585MYS/fold_0/step62.bpnetPipeline/auxiliary/interpret_subsample/modisco_results_profile_scores.h5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff59475-ddb8-4a8e-8390-dbfa53d458a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  prep for step6-2-chrombpnet-pipelline.sh \n",
    "# step 1 : find BAM ID without model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c31781b5-86cd-48d6-bbc2-848ac31c83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os\n",
    "# import csv\n",
    "\n",
    "# def find_bams_ids(base_bams_path):\n",
    "#     \"\"\"List all BAM IDs from the bams directory.\"\"\"\n",
    "#     return [d for d in os.listdir(base_bams_path) if os.path.isdir(os.path.join(base_bams_path, d))]\n",
    "\n",
    "# def find_matching_files_with_glob(base_training_path, target_file_pattern, bam_id):\n",
    "#     \"\"\"Find modisco_results_profile_scores.h5 using glob pattern matching.\"\"\"\n",
    "#     # Correct the glob pattern\n",
    "#     search_pattern = os.path.join(base_training_path, '*', bam_id, '*', 'fold_0', 'step62.bpnetPipeline', 'auxiliary', 'interpret_subsample', target_file_pattern)\n",
    "    \n",
    "#     # Print the search pattern for debugging\n",
    "#     print(f\"Search pattern for {bam_id}: {search_pattern}\")\n",
    "    \n",
    "#     # Use glob to find the files matching the pattern\n",
    "#     found_paths = glob.glob(search_pattern, recursive=True)\n",
    "    \n",
    "#     return found_paths, search_pattern\n",
    "\n",
    "# def compare_files(base_bams_path, base_training_path, target_file, output_file):\n",
    "#     \"\"\"Compare BAM IDs and return those without a matching modisco_results_profile_scores.h5.\"\"\"\n",
    "#     bams_ids = find_bams_ids(base_bams_path)\n",
    "\n",
    "#     # Open CSV file for writing the results\n",
    "#     with open(output_file, mode='w', newline='') as file:\n",
    "#         writer = csv.writer(file)\n",
    "        \n",
    "#         # Write header\n",
    "#         writer.writerow([\"BAM ID\", \"Found Path\", \"Destination Path (Expected)\", \"Match Found?\"])\n",
    "        \n",
    "#         for bam_id in bams_ids:\n",
    "#             # Define the BAM source path\n",
    "#             bam_path = os.path.join(base_bams_path, bam_id)\n",
    "            \n",
    "#             # Find matching files for the target file using glob\n",
    "#             matching_folders, expected_path = find_matching_files_with_glob(base_training_path, target_file, bam_id)\n",
    "            \n",
    "#             if matching_folders:\n",
    "#                 # If matches found, write the actual folder path where the file was found\n",
    "#                 for folder in matching_folders:\n",
    "#                     writer.writerow([bam_id, folder, expected_path, \"Yes\"])\n",
    "#             else:\n",
    "#                 # If no match is found, write the expected search path template\n",
    "#                 writer.writerow([bam_id, \"\", expected_path, \"No\"])\n",
    "            \n",
    "#     print(f\"Results have been written to {output_file}\")\n",
    "\n",
    "# # Define the base paths and target file\n",
    "# base_bams_path = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/bams'\n",
    "# base_training_path = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training'\n",
    "# target_file = 'modisco_results_profile_scores.h5'\n",
    "# output_file = 'missing_models_bams_output.csv'  # Update this to your desired output file location\n",
    "\n",
    "# # Run the comparison and save the output to the file\n",
    "# compare_files(base_bams_path, base_training_path, target_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e53e356-756c-40d4-a2e0-9ce8d1cc2912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing entries have been written to ./steps_inputs/step6/missing_chrombpnet_pipeline_extracted_paths_fold_4.txt\n"
     ]
    }
   ],
   "source": [
    "# step 2: prepare input for the model creation step: step6-2-chrombpnet-pipelline.sh \n",
    "import csv\n",
    "\n",
    "def load_previous_results(previous_results_file):\n",
    "    \"\"\"Load previous results to check which BAM IDs did not match.\"\"\"\n",
    "    missing_bam_ids = set()  # Set to store BAM IDs with no match\n",
    "    with open(previous_results_file, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            bam_id = row[0]  # BAM ID is the first column\n",
    "            match_found = row[3]  # 'Match Found?' is the fourth column\n",
    "            if match_found == \"No\":  # We care about rows where \"Match Found?\" is \"No\"\n",
    "                missing_bam_ids.add(bam_id)\n",
    "    return missing_bam_ids\n",
    "\n",
    "def find_missing_lines(input_file, missing_bam_ids, output_file):\n",
    "    \"\"\"Compare lines in the input file with missing BAM IDs and write relevant ones to the output file.\"\"\"\n",
    "    with open(input_file, mode='r') as infile, open(output_file, mode='w') as outfile:\n",
    "        reader = infile.readlines()\n",
    "        \n",
    "        for line in reader:\n",
    "            # Split the line by space to extract the columns\n",
    "            columns = line.strip().split()\n",
    "            bam_id = columns[1]  # BAM ID is in the second column\n",
    "            \n",
    "            # Check if the BAM ID is in the missing set\n",
    "            if bam_id in missing_bam_ids:\n",
    "                outfile.write(line)  # Write the relevant line to the output file\n",
    "\n",
    "    print(f\"Missing entries have been written to {output_file}\")\n",
    "\n",
    "# Define paths - change the index - based on the fold that you want to execute\n",
    "input_file = './steps_inputs/step6/chrombpnet_pipeline_extracted_paths_fold_4.txt'  # Input text file\n",
    "previous_results_file = 'missing_models_bams_output.csv'  # Previous results file (CSV)\n",
    "output_file = './steps_inputs/step6/missing_chrombpnet_pipeline_extracted_paths_fold_4.txt'  # Corrected Output file path\n",
    "\n",
    "# Load BAM IDs that didn't match from the previous script output\n",
    "missing_bam_ids = load_previous_results(previous_results_file)\n",
    "\n",
    "# Compare input file lines with missing BAM IDs and write missing entries to the output file\n",
    "find_missing_lines(input_file, missing_bam_ids, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b605ca1b-aa4c-4bde-9cc8-ddd10efb4b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# check step 6-3-3 output: how  many models were tested - count the folders:\n",
    "\n",
    " # / debug\n",
    "# !ls -d /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/*/*/*/fold_0/step62.bpnetPipeline/qc/out_step_6_3_3_motifs_qc_bias_tn5 | wc -l\n",
    "\n",
    "# TF atlas:\n",
    "!ls -d /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_0/step62.bpnetPipeline/qc/out_step_6_3_3_motifs_qc_bias_tn5 | wc -l\n",
    "\n",
    "#973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a898477-4014-4fdc-bb4d-6a3b9964249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid logos count: 62176\n",
      "Invalid logos count: 0\n",
      "Total sample count: 973\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the root directory to start the search from\n",
    "root_dir = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/'\n",
    "\n",
    "# Initialize counters for valid and invalid logos\n",
    "valid_logos_count = 0\n",
    "invalid_logos_count = 0\n",
    "\n",
    "# Define the folder patterns for valid and invalid logos\n",
    "valid_folders = ['neg_patterns_valid_logos', 'pos_patterns_valid_logos']\n",
    "invalid_folders = ['neg_patterns_invalid_logos', 'pos_patterns_invalid_logos']\n",
    "\n",
    "\n",
    "# Traverse the directory structure to count directories\n",
    "sample_dirs = glob.glob(f'{root_dir}/*/*/*/fold_0/step62.bpnetPipeline/qc/out_step_6_3_3_motifs_qc_bias_tn5/')\n",
    "\n",
    "# Iterate over the sample directories\n",
    "for sample_dir in sample_dirs:\n",
    "    # Extract the unique ID (the two folders before \"fold_0\")\n",
    "    parts = sample_dir.split('/')\n",
    "    # IDs are the 7th and 8th parts of the path (zero-indexed), i.e., before 'fold_0'\n",
    "    sample_id = '/'.join(parts[-5:-3])  # This gives us the two folders before \"fold_0\"\n",
    "\n",
    "    # Count valid logos\n",
    "    for valid_folder in valid_folders:\n",
    "        valid_folder_path = os.path.join(sample_dir, valid_folder)\n",
    "        if os.path.exists(valid_folder_path):  # Check if folder exists\n",
    "            valid_files = glob.glob(os.path.join(valid_folder_path, '*.png'))\n",
    "            valid_logos_count += len(valid_files)  # Count valid files\n",
    "    \n",
    "    # Count invalid logos\n",
    "    for invalid_folder in invalid_folders:\n",
    "        invalid_folder_path = os.path.join(sample_dir, invalid_folder)\n",
    "        if os.path.exists(invalid_folder_path):  # Check if folder exists\n",
    "            invalid_files = glob.glob(os.path.join(invalid_folder_path, '*.png'))\n",
    "            invalid_logos_count += len(invalid_files)  # Count invalid files\n",
    "\n",
    "# Count the total number of directories in the target folder (equivalent to `ls -d`)\n",
    "total_sample_count = len(sample_dirs)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Valid logos count: {valid_logos_count}\")\n",
    "print(f\"Invalid logos count: {invalid_logos_count}\")\n",
    "print(f\"Total sample count: {total_sample_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57bfee97-4508-4fd4-a121-b74eeaa6263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid logos count: 62176\n",
    "# Invalid logos count: 0\n",
    "# Total sample count: 973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9790b9-ed1f-474e-90b8-36363fe38c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
