{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9aaf9ce-43b6-46a5-a9a9-12537246b8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders in the BAM directory: 1490\n",
      "Number of IDs in the input file: 1490\n",
      "Number of folders not in the input file: 0\n",
      "\n",
      "Folders available in the BAM directory but not in the input file:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Paths to the directory and the input file\n",
    "bam_dir = \"/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/bams/\"\n",
    "input_file = \"./steps_inputs/step1/ENCODE_snatac_pseudobulk_replacements_v2_experiemnt_id_bam_url.txt\"\n",
    "\n",
    "# Get folder names in the BAM directory\n",
    "bam_folders = set(os.listdir(bam_dir))\n",
    "\n",
    "# Read the first column from the input file\n",
    "with open(input_file, \"r\") as file:\n",
    "    file_ids = set(line.split()[0] for line in file if line.strip())  # Extract the first column\n",
    "\n",
    "# Find BAM folders not present in the input file\n",
    "folders_not_in_file = bam_folders - file_ids\n",
    "\n",
    "# Print counts\n",
    "print(f\"Number of folders in the BAM directory: {len(bam_folders)}\")\n",
    "print(f\"Number of IDs in the input file: {len(file_ids)}\")\n",
    "print(f\"Number of folders not in the input file: {len(folders_not_in_file)}\")\n",
    "\n",
    "# Output the results\n",
    "print(\"\\nFolders available in the BAM directory but not in the input file:\")\n",
    "for folder in sorted(folders_not_in_file):\n",
    "    print(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8be8d41c-5c92-4739-b6ef-49c8ed690194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "\n",
    "# # Paths to the directory and the input file\n",
    "# bam_dir = \"/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/bams/\"\n",
    "# input_file = \"./steps_inputs/step1/ENCODE_snatac_pseudobulk_replacements_v2_experiemnt_id_bam_url.txt\"\n",
    "# output_subfile = \"./steps_inputs/step1/missing_bam_files.txt\"  # Output file for missing BAM rows\n",
    "\n",
    "# # Get folder names in the BAM directory (subdirectories are ENCSR_IDs)\n",
    "# bam_folders = {folder for folder in os.listdir(bam_dir) if os.path.isdir(os.path.join(bam_dir, folder))}\n",
    "\n",
    "# # Read the first column from the input file (ENCSR_ID values) and map them to the whole row\n",
    "# input_data = {}\n",
    "# with open(input_file, \"r\") as file:\n",
    "#     for line in file:\n",
    "#         if line.strip():\n",
    "#             columns = line.split()\n",
    "#             encsr_id = columns[0]\n",
    "#             input_data[encsr_id] = line.strip()\n",
    "\n",
    "# # Find ENCSR_IDs from the input file that do not have a corresponding BAM file in the BAM directory\n",
    "# missing_bams = set()\n",
    "\n",
    "# for encsr_id in input_data:\n",
    "#     # Use glob to find the *_unsorted.bam file in the folder\n",
    "#     bam_file_pattern = os.path.join(bam_dir, encsr_id, \"*_unsorted.bam\")\n",
    "#     bam_files = glob.glob(bam_file_pattern)\n",
    "    \n",
    "#     # If no matching BAM file is found, add to missing_bams\n",
    "#     if not bam_files:\n",
    "#         missing_bams.add(encsr_id)  # Corrected here\n",
    "\n",
    "# # Create the sub-file with rows that have missing BAM files\n",
    "# with open(output_subfile, \"w\") as output_file:\n",
    "#     for encsr_id in missing_bams:\n",
    "#         if encsr_id in input_data:\n",
    "#             output_file.write(input_data[encsr_id] + \"\\n\")\n",
    "\n",
    "# # Print counts\n",
    "# print(f\"Number of folders in the BAM directory: {len(bam_folders)}\")\n",
    "# print(f\"Number of IDs in the input file: {len(input_data)}\")\n",
    "# print(f\"Number of BAM folders not in the input file: {len(bam_folders - set(input_data))}\")\n",
    "# print(f\"Number of ENCSR_IDs from the input file missing corresponding BAM files: {len(missing_bams)}\")\n",
    "\n",
    "# # Output the results\n",
    "# print(\"\\nFolders available in the BAM directory but not in the input file:\")\n",
    "# for folder in sorted(bam_folders - set(input_data)):\n",
    "#     print(folder)\n",
    "\n",
    "# print(\"\\nENCSR_IDs from the input file that are missing corresponding BAM files:\")\n",
    "# for encsr_id in sorted(missing_bams):\n",
    "#     print(encsr_id)\n",
    "\n",
    "# print(f\"\\nSub-file with missing BAM files created at: {output_subfile}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ba8c1f-0634-4dc4-9bcf-6ffc429c3f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Define the path to the BAM directory\n",
    "# bam_directory = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/bams/'\n",
    "\n",
    "# # Read the ENCFF IDs from the input file\n",
    "# encff_ids = set()\n",
    "# with open('./steps_inputs/step1/ENCODE_snatac_pseudobulk_replacements_v2_experiemnt_id_bam_url.txt', 'r') as file:\n",
    "#     for line in file:\n",
    "#         # Extract the ENCFF ID from the URL in the second column\n",
    "#         encff_id = line.split()[1].split('/')[-2]\n",
    "#         encff_ids.add(encff_id)\n",
    "\n",
    "# # Iterate through the BAM directory\n",
    "# for folder_name in os.listdir(bam_directory):\n",
    "#     folder_path = os.path.join(bam_directory, folder_name)\n",
    "#     if os.path.isdir(folder_path):\n",
    "#         # List all files in the folder\n",
    "#         for file_name in os.listdir(folder_path):\n",
    "#             if file_name.endswith('_unsorted.bam'):\n",
    "#                 # Extract the ENCFF ID from the file name\n",
    "#                 file_encff_id = file_name.split('_')[0]\n",
    "#                 if file_encff_id not in encff_ids:\n",
    "#                     # Construct the full file path\n",
    "#                     file_path = os.path.join(folder_path, file_name)\n",
    "#                     # Remove the file\n",
    "#                     # os.remove(file_path)\n",
    "#                     print(f'Removed: {file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec7d3f47-b7da-46b2-b5af-e627734d5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat ./steps_inputs/step2/unsorted_bam_files_list-debug.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5a76fb-b726-4351-b7fa-e6b08436b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/bams/ENCSR262XGW/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529661f0-3c39-4cb3-bb97-a52a3cb072ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm ./steps_inputs/step2/missing_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57921c4d-93c0-496e-bd50-01d0410890ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders in the BAM directory: 1490\n",
      "Number of IDs in the input file: 1490\n",
      "Number of BAM folders not in the input file: 0\n",
      "Number of ENCSR_IDs from the input file missing corresponding sorted BAM files: 51\n",
      "\n",
      "Folders available in the BAM directory but not in the input file:\n",
      "\n",
      "ENCSR_IDs and ENCFF_IDs from the input file that are missing corresponding sorted BAM files:\n",
      "ENCSR019MIV - ENCFF851AZC\n",
      "ENCSR024EFJ - ENCFF211OUA\n",
      "ENCSR051INK - ENCFF543NHS\n",
      "ENCSR077YJK - ENCFF361JXN\n",
      "ENCSR115LGZ - ENCFF931GTM\n",
      "ENCSR160NZT - ENCFF822ATI\n",
      "ENCSR279SHN - ENCFF715MBM\n",
      "ENCSR298UDQ - ENCFF906CJI\n",
      "ENCSR322KFP - ENCFF859UNB\n",
      "ENCSR343QDS - ENCFF912PZI\n",
      "ENCSR372JYZ - ENCFF768JOM\n",
      "ENCSR375IBR - ENCFF053VJK\n",
      "ENCSR384XTC - ENCFF122VMK\n",
      "ENCSR400UZX - ENCFF449VSG\n",
      "ENCSR404KPR - ENCFF592FVX\n",
      "ENCSR434LLT - ENCFF949YJW\n",
      "ENCSR497IZS - ENCFF361NAJ\n",
      "ENCSR522ECC - ENCFF462QZS\n",
      "ENCSR530HEB - ENCFF054IYE\n",
      "ENCSR539SHV - ENCFF470EQS\n",
      "ENCSR540IMS - ENCFF339XRM\n",
      "ENCSR554CZQ - ENCFF340EBR\n",
      "ENCSR559HLG - ENCFF958TRE\n",
      "ENCSR583REY - ENCFF921QEX\n",
      "ENCSR602QMJ - ENCFF288GQS\n",
      "ENCSR673UJS - ENCFF668GDG\n",
      "ENCSR709XPF - ENCFF208APD\n",
      "ENCSR730ONS - ENCFF583CXH\n",
      "ENCSR773XZQ - ENCFF475JEL\n",
      "ENCSR777OIM - ENCFF165DFO\n",
      "ENCSR778AYV - ENCFF615TVU\n",
      "ENCSR795ZNC - ENCFF004AYQ\n",
      "ENCSR801YLQ - ENCFF284WBH\n",
      "ENCSR802UHC - ENCFF034YQL\n",
      "ENCSR813YQS - ENCFF533FVP\n",
      "ENCSR814YMN - ENCFF732SZF\n",
      "ENCSR819FPD - ENCFF258NVM\n",
      "ENCSR820SXX - ENCFF336PZL\n",
      "ENCSR844EEK - ENCFF333JWD\n",
      "ENCSR865OGT - ENCFF899ENE\n",
      "ENCSR884EKG - ENCFF368WZI\n",
      "ENCSR884HDM - ENCFF081OCB\n",
      "ENCSR895GCY - ENCFF162JBH\n",
      "ENCSR908VXO - ENCFF712JNU\n",
      "ENCSR930JRX - ENCFF198KXT\n",
      "ENCSR930RZI - ENCFF486FBW\n",
      "ENCSR949YHU - ENCFF733GDJ\n",
      "ENCSR951BES - ENCFF765JRR\n",
      "ENCSR952PWQ - ENCFF646CTW\n",
      "ENCSR961DLS - ENCFF492WFR\n",
      "ENCSR961HRG - ENCFF910HSS\n",
      "\n",
      "Sub-file with missing unsorted BAM file paths created at: ./steps_inputs/step2/missing_bam_unsorted_files.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Paths to the directory and the input file\n",
    "bam_dir = \"/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/bams/\"\n",
    "input_file = \"./steps_inputs/step1/ENCODE_snatac_pseudobulk_replacements_v2_experiemnt_id_bam_url.txt\"\n",
    "output_subfile = \"./steps_inputs/step2/missing_bam_unsorted_files.txt\"  # Output file for missing unsorted BAM rows\n",
    "\n",
    "# Get folder names in the BAM directory (subdirectories are ENCSR_IDs)\n",
    "bam_folders = {folder for folder in os.listdir(bam_dir) if os.path.isdir(os.path.join(bam_dir, folder))}\n",
    "\n",
    "# Read the input file and map the ENCSR_ID and ENCFF_ID (extracted from the URL) to their respective rows\n",
    "input_data = {}\n",
    "with open(input_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            columns = line.split()\n",
    "            encsr_id = columns[0]\n",
    "            encff_url = columns[1]  # Assuming the second column contains the URL\n",
    "\n",
    "            # Extract ENCFF_ID from the URL (after the last '/')\n",
    "            encff_id = encff_url.split('/')[-1].split('.')[0]  # This assumes ENCFF_ID is the part before '.bam'\n",
    "            \n",
    "            input_data[encsr_id] = {'encff_id': encff_id, 'line': line.strip()}\n",
    "\n",
    "# Find ENCSR_IDs from the input file that do not have a corresponding sorted BAM file in the BAM directory\n",
    "missing_sorted_bams = set()\n",
    "\n",
    "for encsr_id, data in input_data.items():\n",
    "    encff_id = data['encff_id']\n",
    "    # Use glob to find the *_sorted.bam file in the folder corresponding to ENCSR_ID\n",
    "    bam_file_pattern = os.path.join(bam_dir, encsr_id, f\"{encff_id}_sorted.bam\")\n",
    "    bam_files = glob.glob(bam_file_pattern)\n",
    "    \n",
    "    # If no matching sorted BAM file is found, add to missing_sorted_bams\n",
    "    if not bam_files:\n",
    "        missing_sorted_bams.add((encsr_id, encff_id))  # Store both ENCSR_ID and ENCFF_ID\n",
    "\n",
    "# Create the sub-file with rows for missing unsorted BAM files\n",
    "with open(output_subfile, \"w\") as output_file:\n",
    "    for encsr_id, encff_id in missing_sorted_bams:\n",
    "        if encsr_id in input_data:\n",
    "            # Construct the path for the missing unsorted BAM file\n",
    "            unsorted_bam_path = os.path.join(bam_dir, encsr_id, f\"{encff_id}_unsorted.bam\")\n",
    "            output_file.write(f\"{unsorted_bam_path}\\n\")  # Write the path to the unsorted BAM file\n",
    "\n",
    "# Print counts\n",
    "print(f\"Number of folders in the BAM directory: {len(bam_folders)}\")\n",
    "print(f\"Number of IDs in the input file: {len(input_data)}\")\n",
    "print(f\"Number of BAM folders not in the input file: {len(bam_folders - set(input_data))}\")\n",
    "print(f\"Number of ENCSR_IDs from the input file missing corresponding sorted BAM files: {len(missing_sorted_bams)}\")\n",
    "\n",
    "# Output the results\n",
    "print(\"\\nFolders available in the BAM directory but not in the input file:\")\n",
    "for folder in sorted(bam_folders - set(input_data)):\n",
    "    print(folder)\n",
    "\n",
    "print(\"\\nENCSR_IDs and ENCFF_IDs from the input file that are missing corresponding sorted BAM files:\")\n",
    "for encsr_id, encff_id in sorted(missing_sorted_bams):\n",
    "    print(f\"{encsr_id} - {encff_id}\")\n",
    "\n",
    "print(f\"\\nSub-file with missing unsorted BAM file paths created at: {output_subfile}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca5932c-d47b-422d-8395-f82f2bbcc293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490\n"
     ]
    }
   ],
   "source": [
    "ls -d $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_data/bams/*/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151d3ea6-d028-4786-a639-5562e581b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce3a44f0-63b8-4688-ae30-12fb1843f7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 validation:\n",
    "\n",
    "# check index and sort files. \n",
    "# check that the files that are assigned to mouse are mouse. compare with ./steps_input/step1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d37c98-61d7-42a1-964a-247b0d94362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691\n"
     ]
    }
   ],
   "source": [
    "!ls -lt $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_negative/*/*/*/fold_0/*nonpeaks_negatives.bed | wc -l\n",
    "#1354\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306d802e-7e41-4125-8297-a09095b23f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lt $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_negative/*/*/*/fold_0/*nonpeaks_negatives.bed | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7043188-2e35-4fcc-b4e3-c76a25cc8ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1354\n"
     ]
    }
   ],
   "source": [
    "!ls -lt $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_negative/*/*/*/fold_1/*nonpeaks_negatives.bed | wc -l\n",
    "#1354\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce206665-cbca-4cae-8a1e-b98514ffc177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1354\n"
     ]
    }
   ],
   "source": [
    "!ls -lt $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_negative/*/*/*/fold_2/*nonpeaks_negatives.bed | wc -l\n",
    "#1354\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d067478-368f-4c45-ba89-9a9400a4cbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1354\n"
     ]
    }
   ],
   "source": [
    "!ls -lt $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_negative/*/*/*/fold_3/*nonpeaks_negatives.bed | wc -l\n",
    "#1354\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0587dc0d-9a4d-4a0f-9b43-3d72e7472bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1354\n"
     ]
    }
   ],
   "source": [
    "!ls -lt $GROUP_SCRATCH/eila/encode_pseudobulks/encode_pseudobulks_negative/*/*/*/fold_4/*nonpeaks_negatives.bed | wc -l\n",
    "#1354\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd88bc54-52f3-40be-ba56-0c2efc55c42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "!squeue --user=$USER | wc -l\n",
    "\n",
    "# 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8eda8f0-742d-48ad-8816-bef15dfa9e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "!squeue --user=$USER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4d75b90-6d61-4ad9-bbe4-8ede68daabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm ./local_logs/slurm_step1_download_bams_out.combined*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec5f63a-e1a6-4c71-b312-59330f37c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scancel -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55c113f-af27-4cec-8d80-736e648b556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# while true; do\n",
    "#     squeue --user=$USER\n",
    "#     sleep 120\n",
    "# done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b763055-e307-47a7-acee-054fbfbb3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scancel 56469701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa34e6-4929-475c-8efe-2b78c288ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm ./local_logs/633*\n",
    "# !./execute_sbatch_arrays_on_sherlock.sh ./steps_inputs/step6_3/debug_one_modisco_results_profile_scores_h5.txt step6-3-3-qc-bias-tn5.sh 1 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aab920bd-90e1-44e2-9643-9d1165d93003",
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"qval\" ./local_logs/slurm.step62.bpnetPipeline.combined.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0381a889-b737-4eb1-8cb4-9d62ac5d5d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory and required folders already exist: /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR110NNR/ENCFF280HZH/fold_1/step62.bpnetPipeline. Exiting.\n",
      "Output directory and required folders already exist: /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR110JCS/ENCFF691JXI/fold_3/step62.bpnetPipeline. Exiting.\n",
      "Output directory and required folders already exist: /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR110NNR/ENCFF280HZH/fold_2/step62.bpnetPipeline. Exiting.\n",
      "Output directory and required folders already exist: /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR110NNR/ENCFF280HZH/fold_3/step62.bpnetPipeline. Exiting.\n",
      "Output directory and required folders already exist: /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR110NNR/ENCFF280HZH/fold_4/step62.bpnetPipeline. Exiting.\n"
     ]
    }
   ],
   "source": [
    "# !wc -l ./steps_inputs/step6/chrombpnet_pipeline_extracted_paths.txt\n",
    "!head -5 ./local_logs/slurm.step62.bpnetPipeline.combined.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da74cf44-253c-4926-b1c8-b6197825d426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory and required folders already exist: /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR607ITW/ENCFF502THJ/fold_4/step62.bpnetPipeline. Exiting.\n"
     ]
    }
   ],
   "source": [
    "!tail -5 ./local_logs/slurm.step62.bpnetPipeline.combined.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f494c4fe-b853-44d5-9072-5f794d5e98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep modisco ./local_logs/slurm.step62.bpnetPipeline.combined.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef5c379-a6aa-4f3a-a1de-cd6fa9745aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"qval\" ./local_logs/slurm.step62.bpnetPipeline.combined.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d8ea066-f383-43b4-83d3-cd417ec62ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ conda shell.bash hook\n",
      "+ eval 'export CONDA_EXE='\\''/home/users/eila/miniconda3/bin/conda'\\''\n",
      "export _CE_M='\\'''\\''\n",
      "export _CE_CONDA='\\'''\\''\n",
      "export CONDA_PYTHON_EXE='\\''/home/users/eila/miniconda3/bin/python'\\''\n"
     ]
    }
   ],
   "source": [
    "!head -5 ./local_logs/slurm.step62.bpnetPipeline.combined.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "965e34bb-8a80-4f99-a68e-daf750d8b14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ for folder in '\"${REQUIRED_FOLDERS[@]}\"'\n",
      "+ '[' '!' -d /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR607ITW/ENCFF502THJ/fold_4/step62.bpnetPipeline/models ']'\n",
      "+ true\n",
      "+ echo 'Output directory and required folders already exist: /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR607ITW/ENCFF502THJ/fold_4/step62.bpnetPipeline. Exiting.'\n",
      "+ exit 0\n"
     ]
    }
   ],
   "source": [
    "!tail -5 ./local_logs/slurm.step62.bpnetPipeline.combined.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "08016a6e-3f6b-44c0-ba36-faf5a13662f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scancel 56235764\n",
    "!scancel -u $USER\n",
    "# !ls ./steps_inputs/step6/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a80fc2f-91ed-4716-b31e-2ddca8cdd593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302\n"
     ]
    }
   ],
   "source": [
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_0/step62.bpnetPipeline/evaluation/overall_report.html | wc -l \n",
    "#1302\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8560dc5-3a7e-46cb-a02f-3c0232be5603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fold 1:\n",
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_1/step62.bpnetPipeline/evaluation/overall_report.html | wc -l \n",
    "# 709\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3dc682d-bdd0-4963-8e4b-730ee1dd335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fold 2:\n",
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_2/step62.bpnetPipeline/evaluation/overall_report.html | wc -l \n",
    "# 474\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759c2450-8f54-4938-abd9-3236346c5155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# fold 3:\n",
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_3/step62.bpnetPipeline/evaluation/overall_report.html | wc -l \n",
    "# 354\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ef58e3-feca-4141-81c9-8fd0464008b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# fold 4:\n",
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_4/step62.bpnetPipeline/evaluation/overall_report.html | wc -l \n",
    "\n",
    "# 377\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759bf51b-b5b9-4540-8bff-c372f36ed811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3216\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# all folds:\n",
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/*/step62.bpnetPipeline/evaluation/overall_report.html | wc -l \n",
    "# 3216\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52247bf1-aad2-4aa5-986e-41c9bba018ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5416/2134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41914edf-9726-441c-a5ae-55a74191c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scancel -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66ce8732-61d7-458c-a6fa-de4c26c5b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !squeue -u $USER -o \"%i %j\" | grep step5Neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "224f0409-6634-4be7-bba2-1303834e7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scp -r eila@login.sherlock.stanford.edu:/scratch/users/eila/encode_pseudobulks_model_training/human/ENCSR037JDN/ENCFF933KCP/fold_1_30000_20240912_182023/evaluation . \n",
    "\n",
    "# !rm ./local_logs/slurm_samools_err.combined.err\n",
    "# !rm ./local_logs/slurm_samools_out.combined.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed9ed22-1c14-4b08-9411-c37a4af0e3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execute_sbatch_arrays_on_sherlock-Copy1.sh\n",
      "execute_sbatch_arrays_on_sherlock-Copy2.sh\n",
      "execute_sbatch_arrays_on_sherlock.sh\n"
     ]
    }
   ],
   "source": [
    "!ls execute*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "042dfbb6-7c20-486b-baff-fe35da1c318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "find /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/*/*/fold_0/step62.bpnetPipeline/evaluation/ -name \"overall_report.html\" \\\n",
    "    | while read dir; do\n",
    "        # Get the parent directory of \"evaluation\"\n",
    "        parent_dir=$(dirname \"$(dirname \"$dir\")\")\n",
    "        \n",
    "        # Check if modisco_results_profile_scores.h5 exists under auxiliary/interpret_subsample\n",
    "        if ! find \"$parent_dir/auxiliary/interpret_subsample\" -name \"modisco_results_profile_scores.h5\" > /dev/null; then\n",
    "            # Print the directory containing overall_report.html if modisco_results_profile_scores.h5 is not found\n",
    "            echo \"$parent_dir\"\n",
    "        fi\n",
    "    done\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06be2332-3a54-4e9a-baba-9e61ec1a065b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories missing at least one of the files ('overall_report.html' or 'modisco_results_profile_scores.h5'):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Set the base directories for the two files\n",
    "evaluation_dir = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/*/*/fold_0/step62.bpnetPipeline/evaluation'\n",
    "modisco_dir = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/*/*/fold_0/step62.bpnetPipeline/auxiliary/interpret_subsample'\n",
    "\n",
    "# Find all directories with the relevant files\n",
    "overall_report_dirs = set(os.path.dirname(path) for path in glob.glob(f\"{evaluation_dir}/overall_report.html\"))\n",
    "modisco_dirs = set(os.path.dirname(path) for path in glob.glob(f\"{modisco_dir}/modisco_results_profile_scores.h5\"))\n",
    "\n",
    "# Get the base path up to fold_0/step62.bpnetPipeline/\n",
    "base_path = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/*/*/fold_0/step62.bpnetPipeline'\n",
    "\n",
    "# Find directories missing at least one file\n",
    "missing_files_dirs = set()\n",
    "\n",
    "# Iterate over directories up to the base path level\n",
    "for dir in glob.glob(f\"{base_path}/*/*/\"):\n",
    "    # If the directory has overall_report.html but not modisco_results_profile_scores.h5\n",
    "    if dir in overall_report_dirs and dir not in modisco_dirs:\n",
    "        missing_files_dirs.add(dir)\n",
    "    # If the directory has modisco_results_profile_scores.h5 but not overall_report.html\n",
    "    elif dir in modisco_dirs and dir not in overall_report_dirs:\n",
    "        missing_files_dirs.add(dir)\n",
    "\n",
    "# Print out the result\n",
    "print(f\"Directories missing at least one of the files ('overall_report.html' or 'modisco_results_profile_scores.h5'):\")\n",
    "for dir in missing_files_dirs:\n",
    "    print(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ce1ea64-b4a7-4529-90cb-3503e3073f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_folders_without_file(base_path, target_file):\n",
    "    # Walk through the directory structure\n",
    "    folders_without_file = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        # Check if the current directory contains the target file\n",
    "        if target_file not in files:\n",
    "            folders_without_file.append(root)\n",
    "    \n",
    "    return folders_without_file\n",
    "\n",
    "# Define the base path and the target file\n",
    "base_path = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_0/'\n",
    "target_file = 'step62.bpnetPipeline/auxiliary/interpret_subsample/modisco_results_profile_scores.h5'\n",
    "\n",
    "# Call the function to find folders without the file\n",
    "folders = find_folders_without_file(base_path, target_file)\n",
    "\n",
    "# Print the results\n",
    "for folder in folders:\n",
    "    print(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5c0a1dc-6670-463d-a1bd-e9ed4b8c7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR000NVR/ENCFF585MYS/fold_0/step62.bpnetPipeline/auxiliary/interpret_subsample/modisco_results_profile_scores.h5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c96e71f-9437-4b85-a614-d16dc38df635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCSR000NVR_sorted.bam\tENCSR000NVR_sorted.bam.bai\n"
     ]
    }
   ],
   "source": [
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/bams/ENCSR000NVR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41fd1199-70af-417a-b76a-88529f1ec3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR000NVR/ENCFF585MYS/fold_0/step62.bpnetPipeline/auxiliary/interpret_subsample/modisco_results_profile_scores.h5\n"
     ]
    }
   ],
   "source": [
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/human/ENCSR000NVR/ENCFF585MYS/fold_0/step62.bpnetPipeline/auxiliary/interpret_subsample/modisco_results_profile_scores.h5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff59475-ddb8-4a8e-8390-dbfa53d458a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  prep for step6-2-chrombpnet-pipelline.sh \n",
    "# step 1 : find BAM ID without model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c31781b5-86cd-48d6-bbc2-848ac31c83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os\n",
    "# import csv\n",
    "\n",
    "# def find_bams_ids(base_bams_path):\n",
    "#     \"\"\"List all BAM IDs from the bams directory.\"\"\"\n",
    "#     return [d for d in os.listdir(base_bams_path) if os.path.isdir(os.path.join(base_bams_path, d))]\n",
    "\n",
    "# def find_matching_files_with_glob(base_training_path, target_file_pattern, bam_id):\n",
    "#     \"\"\"Find modisco_results_profile_scores.h5 using glob pattern matching.\"\"\"\n",
    "#     # Correct the glob pattern\n",
    "#     search_pattern = os.path.join(base_training_path, '*', bam_id, '*', 'fold_0', 'step62.bpnetPipeline', 'auxiliary', 'interpret_subsample', target_file_pattern)\n",
    "    \n",
    "#     # Print the search pattern for debugging\n",
    "#     print(f\"Search pattern for {bam_id}: {search_pattern}\")\n",
    "    \n",
    "#     # Use glob to find the files matching the pattern\n",
    "#     found_paths = glob.glob(search_pattern, recursive=True)\n",
    "    \n",
    "#     return found_paths, search_pattern\n",
    "\n",
    "# def compare_files(base_bams_path, base_training_path, target_file, output_file):\n",
    "#     \"\"\"Compare BAM IDs and return those without a matching modisco_results_profile_scores.h5.\"\"\"\n",
    "#     bams_ids = find_bams_ids(base_bams_path)\n",
    "\n",
    "#     # Open CSV file for writing the results\n",
    "#     with open(output_file, mode='w', newline='') as file:\n",
    "#         writer = csv.writer(file)\n",
    "        \n",
    "#         # Write header\n",
    "#         writer.writerow([\"BAM ID\", \"Found Path\", \"Destination Path (Expected)\", \"Match Found?\"])\n",
    "        \n",
    "#         for bam_id in bams_ids:\n",
    "#             # Define the BAM source path\n",
    "#             bam_path = os.path.join(base_bams_path, bam_id)\n",
    "            \n",
    "#             # Find matching files for the target file using glob\n",
    "#             matching_folders, expected_path = find_matching_files_with_glob(base_training_path, target_file, bam_id)\n",
    "            \n",
    "#             if matching_folders:\n",
    "#                 # If matches found, write the actual folder path where the file was found\n",
    "#                 for folder in matching_folders:\n",
    "#                     writer.writerow([bam_id, folder, expected_path, \"Yes\"])\n",
    "#             else:\n",
    "#                 # If no match is found, write the expected search path template\n",
    "#                 writer.writerow([bam_id, \"\", expected_path, \"No\"])\n",
    "            \n",
    "#     print(f\"Results have been written to {output_file}\")\n",
    "\n",
    "# # Define the base paths and target file\n",
    "# base_bams_path = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_data/bams'\n",
    "# base_training_path = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training'\n",
    "# target_file = 'modisco_results_profile_scores.h5'\n",
    "# output_file = 'missing_models_bams_output.csv'  # Update this to your desired output file location\n",
    "\n",
    "# # Run the comparison and save the output to the file\n",
    "# compare_files(base_bams_path, base_training_path, target_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e53e356-756c-40d4-a2e0-9ce8d1cc2912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing entries have been written to ./steps_inputs/step6/missing_chrombpnet_pipeline_extracted_paths_fold_4.txt\n"
     ]
    }
   ],
   "source": [
    "# step 2: prepare input for the model creation step: step6-2-chrombpnet-pipelline.sh \n",
    "import csv\n",
    "\n",
    "def load_previous_results(previous_results_file):\n",
    "    \"\"\"Load previous results to check which BAM IDs did not match.\"\"\"\n",
    "    missing_bam_ids = set()  # Set to store BAM IDs with no match\n",
    "    with open(previous_results_file, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            bam_id = row[0]  # BAM ID is the first column\n",
    "            match_found = row[3]  # 'Match Found?' is the fourth column\n",
    "            if match_found == \"No\":  # We care about rows where \"Match Found?\" is \"No\"\n",
    "                missing_bam_ids.add(bam_id)\n",
    "    return missing_bam_ids\n",
    "\n",
    "def find_missing_lines(input_file, missing_bam_ids, output_file):\n",
    "    \"\"\"Compare lines in the input file with missing BAM IDs and write relevant ones to the output file.\"\"\"\n",
    "    with open(input_file, mode='r') as infile, open(output_file, mode='w') as outfile:\n",
    "        reader = infile.readlines()\n",
    "        \n",
    "        for line in reader:\n",
    "            # Split the line by space to extract the columns\n",
    "            columns = line.strip().split()\n",
    "            bam_id = columns[1]  # BAM ID is in the second column\n",
    "            \n",
    "            # Check if the BAM ID is in the missing set\n",
    "            if bam_id in missing_bam_ids:\n",
    "                outfile.write(line)  # Write the relevant line to the output file\n",
    "\n",
    "    print(f\"Missing entries have been written to {output_file}\")\n",
    "\n",
    "# Define paths - change the index - based on the fold that you want to execute\n",
    "input_file = './steps_inputs/step6/chrombpnet_pipeline_extracted_paths_fold_4.txt'  # Input text file\n",
    "previous_results_file = 'missing_models_bams_output.csv'  # Previous results file (CSV)\n",
    "output_file = './steps_inputs/step6/missing_chrombpnet_pipeline_extracted_paths_fold_4.txt'  # Corrected Output file path\n",
    "\n",
    "# Load BAM IDs that didn't match from the previous script output\n",
    "missing_bam_ids = load_previous_results(previous_results_file)\n",
    "\n",
    "# Compare input file lines with missing BAM IDs and write missing entries to the output file\n",
    "find_missing_lines(input_file, missing_bam_ids, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b605ca1b-aa4c-4bde-9cc8-ddd10efb4b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# check step 6-3-3 output: how  many models were tested - count the folders:\n",
    "\n",
    " # / debug\n",
    "# !ls -d /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/*/*/*/fold_0/step62.bpnetPipeline/qc/out_step_6_3_3_motifs_qc_bias_tn5 | wc -l\n",
    "\n",
    "# TF atlas:\n",
    "!ls -d /scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/*/*/*/fold_0/step62.bpnetPipeline/qc/out_step_6_3_3_motifs_qc_bias_tn5 | wc -l\n",
    "\n",
    "#973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a898477-4014-4fdc-bb4d-6a3b9964249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid logos count: 62176\n",
      "Invalid logos count: 0\n",
      "Total sample count: 973\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the root directory to start the search from\n",
    "root_dir = '/scratch/groups/akundaje/eila/encode_pseudobulks/encode_pseudobulks_model_training/'\n",
    "\n",
    "# Initialize counters for valid and invalid logos\n",
    "valid_logos_count = 0\n",
    "invalid_logos_count = 0\n",
    "\n",
    "# Define the folder patterns for valid and invalid logos\n",
    "valid_folders = ['neg_patterns_valid_logos', 'pos_patterns_valid_logos']\n",
    "invalid_folders = ['neg_patterns_invalid_logos', 'pos_patterns_invalid_logos']\n",
    "\n",
    "\n",
    "# Traverse the directory structure to count directories\n",
    "sample_dirs = glob.glob(f'{root_dir}/*/*/*/fold_0/step62.bpnetPipeline/qc/out_step_6_3_3_motifs_qc_bias_tn5/')\n",
    "\n",
    "# Iterate over the sample directories\n",
    "for sample_dir in sample_dirs:\n",
    "    # Extract the unique ID (the two folders before \"fold_0\")\n",
    "    parts = sample_dir.split('/')\n",
    "    # IDs are the 7th and 8th parts of the path (zero-indexed), i.e., before 'fold_0'\n",
    "    sample_id = '/'.join(parts[-5:-3])  # This gives us the two folders before \"fold_0\"\n",
    "\n",
    "    # Count valid logos\n",
    "    for valid_folder in valid_folders:\n",
    "        valid_folder_path = os.path.join(sample_dir, valid_folder)\n",
    "        if os.path.exists(valid_folder_path):  # Check if folder exists\n",
    "            valid_files = glob.glob(os.path.join(valid_folder_path, '*.png'))\n",
    "            valid_logos_count += len(valid_files)  # Count valid files\n",
    "    \n",
    "    # Count invalid logos\n",
    "    for invalid_folder in invalid_folders:\n",
    "        invalid_folder_path = os.path.join(sample_dir, invalid_folder)\n",
    "        if os.path.exists(invalid_folder_path):  # Check if folder exists\n",
    "            invalid_files = glob.glob(os.path.join(invalid_folder_path, '*.png'))\n",
    "            invalid_logos_count += len(invalid_files)  # Count invalid files\n",
    "\n",
    "# Count the total number of directories in the target folder (equivalent to `ls -d`)\n",
    "total_sample_count = len(sample_dirs)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Valid logos count: {valid_logos_count}\")\n",
    "print(f\"Invalid logos count: {invalid_logos_count}\")\n",
    "print(f\"Total sample count: {total_sample_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57bfee97-4508-4fd4-a121-b74eeaa6263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid logos count: 62176\n",
    "# Invalid logos count: 0\n",
    "# Total sample count: 973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9790b9-ed1f-474e-90b8-36363fe38c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
