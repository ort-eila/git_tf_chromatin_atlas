{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b74d3a-499d-4736-9c67-69ccde52fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f33e58-a991-477a-8828-dfc7574a987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tn5_motifs(motifs_meme_file):\n",
    "    \"\"\"\n",
    "    Filters TN5 motifs from the given MEME file and writes the filtered motifs to a new MEME file.\n",
    "    Adds the necessary header to the MEME file.\n",
    "    \n",
    "    :param motifs_meme_file: Path to the MEME file containing motifs to be filtered.\n",
    "    :return: Path to the filtered MEME file.\n",
    "    \"\"\"\n",
    "    filtered_motifs = []\n",
    "    \n",
    "    # Step 1: Load motifs from the MEME file\n",
    "    with open(motifs_meme_file, 'r') as f:\n",
    "        motif_data = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"MOTIF\"):  # Start of a new motif\n",
    "                if motif_data:\n",
    "                    filtered_motifs.append(motif_data)\n",
    "                motif_data = [line.strip()]  # Start a new motif\n",
    "            else:\n",
    "                motif_data.append(line.strip())  # Add data to current motif\n",
    "        \n",
    "        # Don't forget to add the last motif\n",
    "        if motif_data:\n",
    "            filtered_motifs.append(motif_data)\n",
    "    \n",
    "    # Step 2: Filter motifs - placeholder for your filtering logic\n",
    "    # Example: Add a simple check, e.g., filtering based on motif name containing \"TN5\"\n",
    "    filtered_motifs = [motif for motif in filtered_motifs if \"TN5\" in motif[0]]  # Adjust this filter as needed\n",
    "    \n",
    "    # Step 3: Write the MEME header and filtered motifs\n",
    "    filtered_motifs_file = motifs_meme_file.replace(\".meme\", \"_filtered.meme\")\n",
    "    \n",
    "    with open(filtered_motifs_file, 'w') as f:\n",
    "        # Write MEME file header\n",
    "        f.write(\"MEME version 4\\n\")\n",
    "        f.write(\"ALPHABET= ACGT\\n\")\n",
    "        f.write(\"strands: + -\\n\")\n",
    "        f.write(\"Background letter frequencies\\n\")\n",
    "        f.write(\"A 0.25 C 0.25 G 0.25 T 0.25\\n\")\n",
    "        \n",
    "        # Add an extra newline after the header\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Write filtered motifs\n",
    "        for motif in filtered_motifs:\n",
    "            for line in motif:\n",
    "                f.write(line + \"\\n\")\n",
    "    \n",
    "    print(f\"Filtered MEME file written to: {filtered_motifs_file}\")\n",
    "    \n",
    "    return filtered_motifs_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efbacda-07f6-495e-8ef4-c32adfcb2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seqlet(seqlet_one_hot):\n",
    "    \"\"\"\n",
    "    Decode the sequence from one-hot encoding. \n",
    "    Assumes the encoding is A=0, C=1, G=2, T=3.\n",
    "    :param seqlet_one_hot: The one-hot encoded sequence (2D array of shape [sequence_length, 4]).\n",
    "    :return: The decoded sequence as a string.\n",
    "    \"\"\"\n",
    "    mapping = ['A', 'C', 'G', 'T']\n",
    "    # For each row (which represents a nucleotide), find the index with value 1 (the base)\n",
    "    return ''.join([mapping[seqlet.argmax()] for seqlet in seqlet_one_hot])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ede9edd-0631-4248-9931-159f662d7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_meme_file(motifs, output_path):\n",
    "    \"\"\"\n",
    "    Writes motifs to a MEME file.\n",
    "    :param motifs: List of tuples [(motif_name, pwm), ...]\n",
    "    :param output_path: Path to the output .meme file.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(\"MEME version 5.0\\n\\n\")\n",
    "        f.write(\"ALPHABET= ACGT\\n\\n\")\n",
    "        f.write(\"strands: + -\\n\\n\")\n",
    "        f.write(\"Background letter frequencies:\\n\")\n",
    "        f.write(\"A 0.25 C 0.25 G 0.25 T 0.25\\n\\n\")\n",
    "        \n",
    "        for motif_name, pwm in motifs:\n",
    "            f.write(f\"MOTIF {motif_name}\\n\")\n",
    "            f.write(f\"letter-probability matrix: alength= 4 w= {pwm.shape[0]}\\n\")\n",
    "            for row in pwm:\n",
    "                f.write(\" \".join(f\"{prob:.6f}\" for prob in row) + \"\\n\")\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1824d49-0b46-4e92-b7dd-bb6267171adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tomtom(query_meme_path, target_meme_path, output_dir):\n",
    "    \"\"\"\n",
    "    Run TOMTOM to compare motifs.\n",
    "    :param query_meme_path: Path to the query motif file in MEME format.\n",
    "    :param target_meme_path: Path to the target motif database in MEME format.\n",
    "    :param output_dir: Path to the output directory to save TOMTOM results.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    command = [\n",
    "        \"tomtom\",\n",
    "        \"-oc\", output_dir,\n",
    "        query_meme_path,\n",
    "        target_meme_ path\n",
    "    ]\n",
    "    try:\n",
    "        print(f\"Calling TOMTOM command {command}\")\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"TOMTOM completed. Results saved in {output_dir}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running TOMTOM: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75cee9e9-a840-4262-bd2f-27632b265110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pwm_from_contrib_scores(contrib_scores):\n",
    "    \"\"\"\n",
    "    Converts contribution scores to a position weight matrix (PWM).\n",
    "    :param contrib_scores: numpy array of shape (L, 4) where L is the sequence length.\n",
    "    :return: PWM as a numpy array of the same shape.\n",
    "    \"\"\"\n",
    "    # Ensure contrib_scores is a numpy array\n",
    "    contrib_scores = np.array(contrib_scores)\n",
    "    \n",
    "    # Normalize scores to obtain probabilities\n",
    "    pwm = np.exp(contrib_scores)  # Exponentiate scores\n",
    "    pwm = pwm / np.sum(pwm, axis=1, keepdims=True)  # Normalize to sum to 1 at each position\n",
    "    \n",
    "    return pwm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1b2749d-e24a-4dab-a450-529fe8d34a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_copy_patterns(input_file, output_dir, group_type, base_logo_dir, threshold, motifs_meme_file):\n",
    "    \"\"\"\n",
    "    Filters motifs based on TOMTOM results and copies the filtered motifs.\n",
    "    :param input_file: Path to the input file (HDF5 file) containing motif data.\n",
    "    :param output_dir: Directory to save filtered motifs.\n",
    "    :param group_type: Type of motifs to process (e.g., 'neg_patterns' or 'pos_patterns').\n",
    "    :param base_logo_dir: Directory containing motif logo images.\n",
    "    :param threshold: Threshold for filtering motifs based on TOMTOM p-value.\n",
    "    :param motifs_meme_file: Path to the MEME file containing TN5 motifs.\n",
    "    \"\"\"\n",
    "    # Step 1: Filter TN5 motifs from the given MEME file\n",
    "    print(f\"Filtering TN5 motifs from {motifs_meme_file}...\")\n",
    "    try:\n",
    "        tn5_filtered_file = filter_tn5_motifs(motifs_meme_file)\n",
    "        print(f\"Filtered TN5 motifs saved to {tn5_filtered_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error filtering TN5 motifs: {e}\")\n",
    "        return\n",
    "\n",
    "    # Define paths for query and target MEME files\n",
    "    query_meme_path = os.path.join(base_logo_dir, f\"{group_type}.meme\")\n",
    "    print(f\"query_meme_path is {query_meme_path}\")\n",
    "    tomtom_output_dir = os.path.join(output_dir, group_type)\n",
    "    print(f\"tomtom_output_dir is {tomtom_output_dir}\")\n",
    "\n",
    "    # Step 2: Read patterns from HDF5 and write them to MEME format (query motifs)\n",
    "    motifs = []\n",
    "    try:\n",
    "        with h5py.File(input_file, 'r') as h5_file:\n",
    "            if group_type not in h5_file:\n",
    "                raise KeyError(f\"Group '{group_type}' not found in HDF5 file.\")\n",
    "\n",
    "            patterns_group = h5_file[group_type]\n",
    "\n",
    "            # Iterate through each pattern and decode seqlets\n",
    "            for pattern_name in patterns_group:\n",
    "                try:\n",
    "                    pattern_group = patterns_group[pattern_name]\n",
    "                    print(f\"Inspecting pattern group for {pattern_name}: {pattern_group}\")\n",
    "\n",
    "                    # Print all keys in the pattern_group for inspection\n",
    "                    # for key in pattern_group.keys():\n",
    "                    #     print(f\"Key: {key}, Value: {pattern_group[key]}\")\n",
    "\n",
    "                    print(f\"Processing pattern: {group_type}/{pattern_name}\")\n",
    "\n",
    "                    # Check if the pattern has seqlets\n",
    "                    seqlets_group = pattern_group.get('seqlets')\n",
    "                    if seqlets_group is None:\n",
    "                        print(f\"Seqlets not found for pattern {pattern_name}. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    contrib_scores = pattern_group['contrib_scores'][()]\n",
    "                    # print(f\"contrib_scores is : {contrib_scores}\")\n",
    "                    pwm = extract_pwm_from_contrib_scores(contrib_scores)\n",
    "                    # print(f\"pwm is : {pwm}\")\n",
    "                    motifs.append((pattern_name, pwm))\n",
    "                    # Print the motifs list after appending the new pattern and seqlet\n",
    "                    # print(f\"Current motifs list: {motifs}\")\n",
    "\n",
    "                except KeyError as e:\n",
    "                    print(f\"Missing key for pattern {pattern_name}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing pattern {pattern_name}: {e}\")\n",
    "\n",
    "        # Step 3: Write the query motifs to MEME format\n",
    "        if motifs:\n",
    "            # print(f\"Motifs to write are: {motifs}\")\n",
    "            print(f\"query_meme_path is: {query_meme_path}\")\n",
    "            write_meme_file(motifs, query_meme_path)\n",
    "            print(f\"Query motifs written to MEME file at {query_meme_path}\")\n",
    "        else:\n",
    "            print(f\"No valid motifs found for {group_type}. Skipping TOMTOM.\")\n",
    "            return\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading from HDF5 file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 4: Run TOMTOM to compare the query and target motifs\n",
    "    try:\n",
    "        print(f\"Running TOMTOM for {group_type} motifs...\")\n",
    "        run_tomtom(query_meme_path, tn5_filtered_file, tomtom_output_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running TOMTOM: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 5: Parse TOMTOM results and filter based on p-value threshold\n",
    "    tomtom_results_path = os.path.join(tomtom_output_dir, \"tomtom.tsv\")\n",
    "    print(f\"tomtom_results_path is {tomtom_results_path}\")\n",
    "    \n",
    "    try:\n",
    "        tomtom_results = pd.read_csv(\n",
    "            tomtom_results_path,\n",
    "            sep=\"\\t\",\n",
    "            comment=\"#\",\n",
    "            usecols=[\"Query_ID\", \"Target_ID\", \"p-value\", \"q-value\", \"E-value\"]\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"TOMTOM results not found for {group_type}. Skipping.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading TOMTOM results: {e}\")\n",
    "        return\n",
    "\n",
    "    # Filter motifs based on the p-value threshold\n",
    "    filtered_motifs = tomtom_results[tomtom_results[\"p-value\"] <= (threshold / 100.0)]\n",
    "\n",
    "    if filtered_motifs.empty:\n",
    "        print(f\"No motifs passed the threshold for {group_type}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Filtered motifs for {group_type}:\")\n",
    "    print(filtered_motifs)\n",
    "\n",
    "    # Step 6: Copy filtered motifs' logo images to output directory\n",
    "    for motif_id in filtered_motifs[\"Query_ID\"].unique():\n",
    "        src_path = os.path.join(base_logo_dir, f\"{motif_id}.png\")\n",
    "        dest_path = os.path.join(tomtom_output_dir, f\"{motif_id}.png\")\n",
    "\n",
    "        # Check if the logo image exists before copying\n",
    "        if os.path.exists(src_path):\n",
    "            subprocess.run([\"cp\", src_path, dest_path])\n",
    "            print(f\"Copied {motif_id}.png to {dest_path}\")\n",
    "        else:\n",
    "            print(f\"Logo image for {motif_id} not found. Skipping.\")\n",
    "\n",
    "    print(f\"Filtered motifs for {group_type} have been processed and copied.\")\n",
    "\n",
    "    # Step 7: Save filtered motifs as a CSV file for later reference\n",
    "    filtered_motifs_csv = os.path.join(tomtom_output_dir, f\"{group_type}_filtered_motifs.csv\")\n",
    "    filtered_motifs.to_csv(filtered_motifs_csv, index=False)\n",
    "    print(f\"Filtered motifs saved to {filtered_motifs_csv}\")\n",
    "\n",
    "    print(f\"Motif processing for {group_type} is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a84a8559-7ea2-483d-be95-688ac25f33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to provided motifs.meme file\n",
    "motifs_meme_file = \"./steps_inputs/step6_3/motifs.meme\"\n",
    "\n",
    "# Threshold for motif filtering\n",
    "threshold = 80  # Adjust as necessary\n",
    "\n",
    "# Input file path\n",
    "input_file_path = \"/scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/auxiliary/interpret_subsample/modisco_results_profile_scores.h5\"\n",
    "\n",
    "# Extracting SPECIES, ENCSR_id, and ENCFF_id from input_file_path (optional, if you want to extract dynamically)\n",
    "# Using hardcoded values as given\n",
    "SPECIES = \"human\"\n",
    "ENCSR_id = \"ENCSR004DZS\"\n",
    "ENCFF_id = \"ENCFF709MRG\"\n",
    "\n",
    "# Base directory for logos and output\n",
    "base_logo_dir = f\"/scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/{SPECIES}/{ENCSR_id}/{ENCFF_id}/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/\"\n",
    "output_dir = f\"/scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/{SPECIES}/{ENCSR_id}/{ENCFF_id}/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82371c24-5ea4-4f3d-8c46-8958ab693bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "842032f4-5abc-4b01-98bb-5fc029e7e7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_logo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf1d948e-27bf-408b-a87e-09bb9f7d36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tn5_filtered_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "517a66db-a0fb-4fc2-a7fa-7b329e43913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing neg_patterns with threshold 80...\n",
      "Filtering TN5 motifs from ./steps_inputs/step6_3/motifs.meme...\n",
      "Filtered MEME file written to: ./steps_inputs/step6_3/motifs_filtered.meme\n",
      "Filtered TN5 motifs saved to ./steps_inputs/step6_3/motifs_filtered.meme\n",
      "query_meme_path is /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/neg_patterns.meme\n",
      "tomtom_output_dir is /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/neg_patterns\n",
      "Inspecting pattern group for pattern_0: <HDF5 group \"/neg_patterns/pattern_0\" (8 members)>\n",
      "Processing pattern: neg_patterns/pattern_0\n",
      "Inspecting pattern group for pattern_1: <HDF5 group \"/neg_patterns/pattern_1\" (5 members)>\n",
      "Processing pattern: neg_patterns/pattern_1\n",
      "query_meme_path is: /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/neg_patterns.meme\n",
      "Query motifs written to MEME file at /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/neg_patterns.meme\n",
      "Running TOMTOM for neg_patterns motifs...\n",
      "Calling TOMTOM command ['tomtom', '-oc', '/scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/neg_patterns', '/scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/neg_patterns.meme', './steps_inputs/step6_3/motifs_filtered.meme']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Target database size too small (8) for accurate p-value computation.\n",
      "Provide at least 50 motifs for accurate p-value computation.\n",
      "The output directory '/scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/neg_patterns' already exists.\n",
      "Its contents will be overwritten.\n",
      "Processing query 1 out of 2 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 2 out of 2 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOMTOM completed. Results saved in /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/neg_patterns\n",
      "tomtom_results_path is /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/neg_patterns/tomtom.tsv\n",
      "Filtered motifs for neg_patterns:\n",
      "    Query_ID Target_ID   p-value   E-value   q-value\n",
      "0  pattern_0     TN5_1  0.000130  0.001038  0.001117\n",
      "1  pattern_1     TN5_1  0.000141  0.001124  0.001124\n",
      "Logo image for pattern_0 not found. Skipping.\n",
      "Logo image for pattern_1 not found. Skipping.\n",
      "Filtered motifs for neg_patterns have been processed and copied.\n",
      "Filtered motifs saved to /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/neg_patterns/neg_patterns_filtered_motifs.csv\n",
      "Motif processing for neg_patterns is complete.\n",
      "Processing pos_patterns with threshold 80...\n",
      "Filtering TN5 motifs from ./steps_inputs/step6_3/motifs.meme...\n",
      "Filtered MEME file written to: ./steps_inputs/step6_3/motifs_filtered.meme\n",
      "Filtered TN5 motifs saved to ./steps_inputs/step6_3/motifs_filtered.meme\n",
      "query_meme_path is /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/pos_patterns.meme\n",
      "tomtom_output_dir is /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/pos_patterns\n",
      "Inspecting pattern group for pattern_0: <HDF5 group \"/pos_patterns/pattern_0\" (21 members)>\n",
      "Processing pattern: pos_patterns/pattern_0\n",
      "Inspecting pattern group for pattern_1: <HDF5 group \"/pos_patterns/pattern_1\" (17 members)>\n",
      "Processing pattern: pos_patterns/pattern_1\n",
      "Inspecting pattern group for pattern_10: <HDF5 group \"/pos_patterns/pattern_10\" (9 members)>\n",
      "Processing pattern: pos_patterns/pattern_10\n",
      "Inspecting pattern group for pattern_11: <HDF5 group \"/pos_patterns/pattern_11\" (12 members)>\n",
      "Processing pattern: pos_patterns/pattern_11\n",
      "Inspecting pattern group for pattern_12: <HDF5 group \"/pos_patterns/pattern_12\" (9 members)>\n",
      "Processing pattern: pos_patterns/pattern_12\n",
      "Inspecting pattern group for pattern_13: <HDF5 group \"/pos_patterns/pattern_13\" (9 members)>\n",
      "Processing pattern: pos_patterns/pattern_13\n",
      "Inspecting pattern group for pattern_14: <HDF5 group \"/pos_patterns/pattern_14\" (8 members)>\n",
      "Processing pattern: pos_patterns/pattern_14\n",
      "Inspecting pattern group for pattern_15: <HDF5 group \"/pos_patterns/pattern_15\" (5 members)>\n",
      "Processing pattern: pos_patterns/pattern_15\n",
      "Inspecting pattern group for pattern_16: <HDF5 group \"/pos_patterns/pattern_16\" (5 members)>\n",
      "Processing pattern: pos_patterns/pattern_16\n",
      "Inspecting pattern group for pattern_17: <HDF5 group \"/pos_patterns/pattern_17\" (5 members)>\n",
      "Processing pattern: pos_patterns/pattern_17\n",
      "Inspecting pattern group for pattern_18: <HDF5 group \"/pos_patterns/pattern_18\" (5 members)>\n",
      "Processing pattern: pos_patterns/pattern_18\n",
      "Inspecting pattern group for pattern_2: <HDF5 group \"/pos_patterns/pattern_2\" (14 members)>\n",
      "Processing pattern: pos_patterns/pattern_2\n",
      "Inspecting pattern group for pattern_3: <HDF5 group \"/pos_patterns/pattern_3\" (10 members)>\n",
      "Processing pattern: pos_patterns/pattern_3\n",
      "Inspecting pattern group for pattern_4: <HDF5 group \"/pos_patterns/pattern_4\" (18 members)>\n",
      "Processing pattern: pos_patterns/pattern_4\n",
      "Inspecting pattern group for pattern_5: <HDF5 group \"/pos_patterns/pattern_5\" (12 members)>\n",
      "Processing pattern: pos_patterns/pattern_5\n",
      "Inspecting pattern group for pattern_6: <HDF5 group \"/pos_patterns/pattern_6\" (11 members)>\n",
      "Processing pattern: pos_patterns/pattern_6\n",
      "Inspecting pattern group for pattern_7: <HDF5 group \"/pos_patterns/pattern_7\" (8 members)>\n",
      "Processing pattern: pos_patterns/pattern_7\n",
      "Inspecting pattern group for pattern_8: <HDF5 group \"/pos_patterns/pattern_8\" (13 members)>\n",
      "Processing pattern: pos_patterns/pattern_8\n",
      "Inspecting pattern group for pattern_9: <HDF5 group \"/pos_patterns/pattern_9\" (11 members)>\n",
      "Processing pattern: pos_patterns/pattern_9\n",
      "query_meme_path is: /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/pos_patterns.meme\n",
      "Query motifs written to MEME file at /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/pos_patterns.meme\n",
      "Running TOMTOM for pos_patterns motifs...\n",
      "Calling TOMTOM command ['tomtom', '-oc', '/scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/pos_patterns', '/scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/pos_patterns.meme', './steps_inputs/step6_3/motifs_filtered.meme']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Target database size too small (8) for accurate p-value computation.\n",
      "Provide at least 50 motifs for accurate p-value computation.\n",
      "The output directory '/scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/pos_patterns' already exists.\n",
      "Its contents will be overwritten.\n",
      "Processing query 1 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 2 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 3 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 4 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 5 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 6 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 7 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 8 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 9 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 10 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 11 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 12 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 13 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 14 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 15 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 16 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 17 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 18 out of 19 \n",
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n",
      "Processing query 19 out of 19 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOMTOM completed. Results saved in /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/pos_patterns\n",
      "tomtom_results_path is /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/pos_patterns/tomtom.tsv\n",
      "Filtered motifs for pos_patterns:\n",
      "      Query_ID Target_ID   p-value   E-value   q-value\n",
      "0    pattern_0     TN5_1  0.000133  0.001061  0.001061\n",
      "1    pattern_1     TN5_1  0.000134  0.001075  0.001075\n",
      "2   pattern_10     TN5_1  0.000132  0.001058  0.001138\n",
      "3   pattern_11     TN5_1  0.000133  0.001062  0.001062\n",
      "4   pattern_12     TN5_1  0.000134  0.001073  0.001073\n",
      "5   pattern_13     TN5_1  0.000133  0.001063  0.001063\n",
      "6   pattern_14     TN5_1  0.000133  0.001063  0.001063\n",
      "7   pattern_15     TN5_1  0.000132  0.001058  0.001139\n",
      "8   pattern_16     TN5_1  0.000133  0.001060  0.001060\n",
      "9   pattern_17     TN5_1  0.000133  0.001062  0.001143\n",
      "10  pattern_18     TN5_1  0.000140  0.001120  0.001120\n",
      "11   pattern_2     TN5_1  0.000132  0.001057  0.001057\n",
      "12   pattern_3     TN5_1  0.000139  0.001111  0.001196\n",
      "13   pattern_4     TN5_1  0.000133  0.001066  0.001147\n",
      "14   pattern_5     TN5_1  0.000136  0.001087  0.001087\n",
      "15   pattern_6     TN5_1  0.000134  0.001075  0.001075\n",
      "16   pattern_7     TN5_1  0.000142  0.001139  0.001139\n",
      "17   pattern_8     TN5_1  0.000133  0.001061  0.001061\n",
      "18   pattern_9     TN5_1  0.000135  0.001077  0.001077\n",
      "Logo image for pattern_0 not found. Skipping.\n",
      "Logo image for pattern_1 not found. Skipping.\n",
      "Logo image for pattern_10 not found. Skipping.\n",
      "Logo image for pattern_11 not found. Skipping.\n",
      "Logo image for pattern_12 not found. Skipping.\n",
      "Logo image for pattern_13 not found. Skipping.\n",
      "Logo image for pattern_14 not found. Skipping.\n",
      "Logo image for pattern_15 not found. Skipping.\n",
      "Logo image for pattern_16 not found. Skipping.\n",
      "Logo image for pattern_17 not found. Skipping.\n",
      "Logo image for pattern_18 not found. Skipping.\n",
      "Logo image for pattern_2 not found. Skipping.\n",
      "Logo image for pattern_3 not found. Skipping.\n",
      "Logo image for pattern_4 not found. Skipping.\n",
      "Logo image for pattern_5 not found. Skipping.\n",
      "Logo image for pattern_6 not found. Skipping.\n",
      "Logo image for pattern_7 not found. Skipping.\n",
      "Logo image for pattern_8 not found. Skipping.\n",
      "Logo image for pattern_9 not found. Skipping.\n",
      "Filtered motifs for pos_patterns have been processed and copied.\n",
      "Filtered motifs saved to /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/pos_patterns/pos_patterns_filtered_motifs.csv\n",
      "Motif processing for pos_patterns is complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Computing q-values.\n",
      "#   Cannot estimate pi_0 accurately from fewer than 100 p-values.\n",
      "#   Total p-values = 16. Using pi_zero = 1.0.\n"
     ]
    }
   ],
   "source": [
    "# Define the group types\n",
    "group_types = ['neg_patterns', 'pos_patterns']\n",
    "\n",
    "# Loop through the group types and run the processing\n",
    "for group_type in group_types:\n",
    "    print(f\"Processing {group_type} with threshold {threshold}...\")\n",
    "    \n",
    "    # Call the function directly from the notebook\n",
    "    filter_and_copy_patterns(input_file_path, output_dir, group_type, base_logo_dir, threshold, motifs_meme_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fab965e-207b-4148-a518-6613f1b6103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/pos_patterns.meme ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "defdcdf6-22e8-470f-8b08-c514c1e97bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/neg_patterns.meme ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c24b57b-81a8-4c3b-ab35-6d1772eea189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_patterns.meme\t\t     pos_patterns.pattern_17.cwm.fwd.png\n",
      "neg_patterns.pattern_0.cwm.fwd.png   pos_patterns.pattern_17.cwm.rev.png\n",
      "neg_patterns.pattern_0.cwm.rev.png   pos_patterns.pattern_18.cwm.fwd.png\n",
      "neg_patterns.pattern_1.cwm.fwd.png   pos_patterns.pattern_18.cwm.rev.png\n",
      "neg_patterns.pattern_1.cwm.rev.png   pos_patterns.pattern_1.cwm.fwd.png\n",
      "pos_patterns.meme\t\t     pos_patterns.pattern_1.cwm.rev.png\n",
      "pos_patterns.pattern_0.cwm.fwd.png   pos_patterns.pattern_2.cwm.fwd.png\n",
      "pos_patterns.pattern_0.cwm.rev.png   pos_patterns.pattern_2.cwm.rev.png\n",
      "pos_patterns.pattern_10.cwm.fwd.png  pos_patterns.pattern_3.cwm.fwd.png\n",
      "pos_patterns.pattern_10.cwm.rev.png  pos_patterns.pattern_3.cwm.rev.png\n",
      "pos_patterns.pattern_11.cwm.fwd.png  pos_patterns.pattern_4.cwm.fwd.png\n",
      "pos_patterns.pattern_11.cwm.rev.png  pos_patterns.pattern_4.cwm.rev.png\n",
      "pos_patterns.pattern_12.cwm.fwd.png  pos_patterns.pattern_5.cwm.fwd.png\n",
      "pos_patterns.pattern_12.cwm.rev.png  pos_patterns.pattern_5.cwm.rev.png\n",
      "pos_patterns.pattern_13.cwm.fwd.png  pos_patterns.pattern_6.cwm.fwd.png\n",
      "pos_patterns.pattern_13.cwm.rev.png  pos_patterns.pattern_6.cwm.rev.png\n",
      "pos_patterns.pattern_14.cwm.fwd.png  pos_patterns.pattern_7.cwm.fwd.png\n",
      "pos_patterns.pattern_14.cwm.rev.png  pos_patterns.pattern_7.cwm.rev.png\n",
      "pos_patterns.pattern_15.cwm.fwd.png  pos_patterns.pattern_8.cwm.fwd.png\n",
      "pos_patterns.pattern_15.cwm.rev.png  pos_patterns.pattern_8.cwm.rev.png\n",
      "pos_patterns.pattern_16.cwm.fwd.png  pos_patterns.pattern_9.cwm.fwd.png\n",
      "pos_patterns.pattern_16.cwm.rev.png  pos_patterns.pattern_9.cwm.rev.png\n"
     ]
    }
   ],
   "source": [
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7740def6-2de4-43ef-a6d1-1768fd5a4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/evaluation/modisco_profile/trimmed_logos/pos_patterns.meme .\n",
    "\n",
    "# cp pos_patterns.meme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db5b68df-1954-4568-818e-134e3eaa3aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_patterns_filtered_motifs.csv  tomtom.html  tomtom.tsv  tomtom.xml\n"
     ]
    }
   ],
   "source": [
    "!ls /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/neg_patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daba7a0b-77eb-4d13-8cf2-d5445ed5d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /scratch/groups/akundaje/eila/encode_pseudobulks/old_encode_pseudobulks_model_training/human/ENCSR004DZS/ENCFF709MRG/fold_0/step62.bpnetPipeline/qc/out_step_6_3_2_tomtom/neg_patterns/* .\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f7e4c-aa76-407b-b34e-04bbf6c51ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chrombpnet",
   "language": "python",
   "name": "chrombpnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
